{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560fad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 1: LOGISTIC REGRESSION - COMPLETE TRAINING PIPELINE\n",
    "# ============================================================================\n",
    "# This cell contains the entire Model 1 workflow:\n",
    "# - Data loading and preprocessing\n",
    "# - Feature engineering \n",
    "# - Model training (Logistic Regression)\n",
    "# - Evaluation and model saving\n",
    "# Collapse this cell when working on other models\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"ðŸš€ Starting Model 1: Logistic Regression Training Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========== Step 1: Load dataset ==========\n",
    "print(\"ðŸ“‚ Loading dataset...\")\n",
    "file_path = \"features_10000.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "df = df.rename(columns={\"type\": \"label\"})\n",
    "\n",
    "# Balance dataset: 2500 benign, 1500 malicious\n",
    "benign_df = df[df[\"label\"].str.lower() == \"benign\"].head(2500)\n",
    "malicious_df = df[df[\"label\"].str.lower() == \"malicious\"].head(1500)\n",
    "sample_df = pd.concat([benign_df, malicious_df]).reset_index(drop=True)\n",
    "print(f\"âœ… Dataset loaded: {len(sample_df)} samples ({len(benign_df)} benign, {len(malicious_df)} malicious)\")\n",
    "\n",
    "# ========== Step 2: Feature Engineering ==========\n",
    "print(\"\\nðŸ”§ Extracting features...\")\n",
    "def extract_features(url):\n",
    "    try:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "    except:\n",
    "        parsed = None\n",
    "\n",
    "    url_length = len(url)\n",
    "    num_dots = url.count(\".\")\n",
    "    num_hyphens = url.count(\"-\")\n",
    "    num_at = url.count(\"@\")\n",
    "    num_digits = sum(c.isdigit() for c in url)\n",
    "    num_params = url.count(\"=\")\n",
    "    num_slashes = url.count(\"/\")\n",
    "    num_question = url.count(\"?\")\n",
    "    num_percent = url.count(\"%\")\n",
    "    num_special = sum(c in [';', '_', '?', '=', '&'] for c in url)\n",
    "\n",
    "    hostname = parsed.hostname if parsed and parsed.hostname else \"\"\n",
    "    domain_length = len(hostname)\n",
    "\n",
    "    path_length = len(parsed.path) if parsed and parsed.path else 0\n",
    "    has_https = 1 if parsed and parsed.scheme == \"https\" else 0\n",
    "    has_http = 1 if parsed and parsed.scheme == \"http\" else 0\n",
    "\n",
    "    keywords = [\"login\", \"secure\", \"update\", \"free\", \"verify\", \"bank\", \"account\", \"paypal\"]\n",
    "    has_suspicious_kw = any(kw in url.lower() for kw in keywords)\n",
    "\n",
    "    return [\n",
    "        url_length, num_dots, num_hyphens, num_at, num_digits,\n",
    "        num_params, num_slashes, num_question, num_percent, num_special,\n",
    "        domain_length, path_length, has_https, has_http,\n",
    "        has_suspicious_kw\n",
    "    ]\n",
    "\n",
    "# Apply feature extraction\n",
    "features = sample_df[\"url\"].apply(extract_features)\n",
    "feature_names = [\n",
    "    \"url_length\", \"num_dots\", \"num_hyphens\", \"num_at\", \"num_digits\",\n",
    "    \"num_params\", \"num_slashes\", \"num_question\", \"num_percent\", \"num_special\",\n",
    "    \"domain_length\", \"path_length\", \"has_https\", \"has_http\",\n",
    "    \"has_suspicious_kw\"\n",
    "]\n",
    "features_df = pd.DataFrame(features.tolist(), columns=feature_names)\n",
    "\n",
    "# Merge with URL + label\n",
    "final_df = pd.concat([sample_df[\"url\"].reset_index(drop=True),\n",
    "                      features_df,\n",
    "                      sample_df[\"label\"].reset_index(drop=True)], axis=1)\n",
    "print(f\"âœ… Features extracted: {len(feature_names)} features\")\n",
    "\n",
    "# ========== Step 3: Prepare Data ==========\n",
    "print(\"\\nðŸ“Š Preparing training data...\")\n",
    "X = final_df.drop(columns=[\"url\", \"label\"])\n",
    "y = final_df[\"label\"].map({\"benign\": 0, \"malicious\": 1})\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(f\"âœ… Data prepared: {len(X_train)} training, {len(X_test)} test samples\")\n",
    "\n",
    "# ========== Step 4: Train Logistic Regression ==========\n",
    "print(\"\\nðŸ¤– Training Logistic Regression model...\")\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"âœ… Model training completed!\")\n",
    "\n",
    "# ========== Step 5: Evaluate ==========\n",
    "print(\"\\nðŸ“ˆ Evaluating model performance...\")\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"coefficient\": model.coef_[0]\n",
    "}).sort_values(by=\"coefficient\", ascending=False)\n",
    "\n",
    "print(\"\\nðŸ”Ž Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# ========== Step 6: Save Models ==========\n",
    "print(\"\\nðŸ’¾ Saving trained models...\")\n",
    "\n",
    "# Save the trained logistic regression model\n",
    "with open('logistic_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Save the scaler\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save the label encoder mapping (for reference)\n",
    "label_mapping = {\"benign\": 0, \"malicious\": 1}\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_mapping, f)\n",
    "\n",
    "print(\"âœ… Models saved successfully!\")\n",
    "print(\"- logistic_model.pkl: Trained logistic regression model\")\n",
    "print(\"- scaler.pkl: Feature scaler\")\n",
    "print(\"- label_encoder.pkl: Label mapping\")\n",
    "\n",
    "# Display model summary\n",
    "print(f\"\\nðŸ“Š Model 1 Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Model Type: Logistic Regression\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features: {len(feature_names)}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"=\" * 40)\n",
    "print(\"ðŸŽ‰ Model 1 training pipeline completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5606dcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Model 2: Logistic Regression Training Pipeline (No HTTPS Feature)\n",
      "======================================================================\n",
      "ðŸ“‚ Loading dataset...\n",
      "âœ… Dataset loaded: 4000 samples (2500 benign, 1500 malicious)\n",
      "\n",
      "ðŸ”§ Extracting features (excluding has_https)...\n",
      "âœ… Features extracted: 14 features (dropped has_https)\n",
      "\n",
      "ðŸ“Š Preparing training data...\n",
      "âœ… Data prepared: 3200 training, 800 test samples\n",
      "\n",
      "ðŸ¤– Training Logistic Regression model...\n",
      "âœ… Model training completed!\n",
      "\n",
      "ðŸ“ˆ Evaluating model performance...\n",
      "âœ… Logistic Regression Accuracy (No HTTPS): 0.9263\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94       500\n",
      "           1       0.94      0.86      0.90       300\n",
      "\n",
      "    accuracy                           0.93       800\n",
      "   macro avg       0.93      0.91      0.92       800\n",
      "weighted avg       0.93      0.93      0.93       800\n",
      "\n",
      "\n",
      "ðŸ”Ž Feature Importance:\n",
      "              feature  coefficient\n",
      "10      domain_length     4.229212\n",
      "12           has_http     2.956892\n",
      "5          num_params     0.880565\n",
      "1            num_dots     0.303946\n",
      "13  has_suspicious_kw     0.265099\n",
      "8         num_percent    -0.072192\n",
      "4          num_digits    -0.101870\n",
      "3              num_at    -0.173407\n",
      "11        path_length    -0.458502\n",
      "7        num_question    -0.493626\n",
      "0          url_length    -0.743348\n",
      "2         num_hyphens    -0.976631\n",
      "9         num_special    -1.305162\n",
      "6         num_slashes    -2.260823\n",
      "\n",
      "ðŸ’¾ Saving trained models...\n",
      "âœ… Models saved successfully!\n",
      "- logistic_model_v2.pkl: Trained logistic regression model (no HTTPS)\n",
      "- scaler_v2.pkl: Feature scaler\n",
      "- label_encoder_v2.pkl: Label mapping\n",
      "\n",
      "ðŸ“Š Model 2 Summary:\n",
      "========================================\n",
      "Model Type: Logistic Regression (No HTTPS)\n",
      "Training samples: 3200\n",
      "Test samples: 800\n",
      "Features: 14 (dropped has_https)\n",
      "Test Accuracy: 0.9263\n",
      "========================================\n",
      "ðŸŽ‰ Model 2 training pipeline completed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL 2: LOGISTIC REGRESSION WITHOUT HTTPS FEATURE - COMPLETE TRAINING PIPELINE\n",
    "# ============================================================================\n",
    "# This cell contains the entire Model 2 workflow (similar to Model 1 but drops has_https):\n",
    "# - Data loading and preprocessing\n",
    "# - Feature engineering (14 features instead of 15)\n",
    "# - Model training (Logistic Regression)\n",
    "# - Evaluation and model saving\n",
    "# Collapse this cell when working on other models\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"ðŸš€ Starting Model 2: Logistic Regression Training Pipeline (No HTTPS Feature)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========== Step 1: Load dataset ==========\n",
    "print(\"ðŸ“‚ Loading dataset...\")\n",
    "file_path = \"features_10000.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "df = df.rename(columns={\"type\": \"label\"})\n",
    "\n",
    "# Balance dataset: 2500 benign, 1500 malicious\n",
    "benign_df = df[df[\"label\"].str.lower() == \"benign\"].head(2500)\n",
    "malicious_df = df[df[\"label\"].str.lower() == \"malicious\"].head(1500)\n",
    "sample_df = pd.concat([benign_df, malicious_df]).reset_index(drop=True)\n",
    "print(f\"âœ… Dataset loaded: {len(sample_df)} samples ({len(benign_df)} benign, {len(malicious_df)} malicious)\")\n",
    "\n",
    "# ========== Step 2: Feature Engineering (WITHOUT has_https) ==========\n",
    "print(\"\\nðŸ”§ Extracting features (excluding has_https)...\")\n",
    "def extract_features_no_https(url):\n",
    "    try:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "    except:\n",
    "        parsed = None\n",
    "\n",
    "    url_length = len(url)\n",
    "    num_dots = url.count(\".\")\n",
    "    num_hyphens = url.count(\"-\")\n",
    "    num_at = url.count(\"@\")\n",
    "    num_digits = sum(c.isdigit() for c in url)\n",
    "    num_params = url.count(\"=\")\n",
    "    num_slashes = url.count(\"/\")\n",
    "    num_question = url.count(\"?\")\n",
    "    num_percent = url.count(\"%\")\n",
    "    num_special = sum(c in [';', '_', '?', '=', '&'] for c in url)\n",
    "\n",
    "    hostname = parsed.hostname if parsed and parsed.hostname else \"\"\n",
    "    domain_length = len(hostname)\n",
    "\n",
    "    path_length = len(parsed.path) if parsed and parsed.path else 0\n",
    "    has_http = 1 if parsed and parsed.scheme == \"http\" else 0\n",
    "\n",
    "    keywords = [\"login\", \"secure\", \"update\", \"free\", \"verify\", \"bank\", \"account\", \"paypal\"]\n",
    "    has_suspicious_kw = any(kw in url.lower() for kw in keywords)\n",
    "\n",
    "    # NOTE: Removed has_https feature\n",
    "    return [\n",
    "        url_length, num_dots, num_hyphens, num_at, num_digits,\n",
    "        num_params, num_slashes, num_question, num_percent, num_special,\n",
    "        domain_length, path_length, has_http,\n",
    "        has_suspicious_kw\n",
    "    ]\n",
    "\n",
    "# Apply feature extraction\n",
    "features = sample_df[\"url\"].apply(extract_features_no_https)\n",
    "feature_names = [\n",
    "    \"url_length\", \"num_dots\", \"num_hyphens\", \"num_at\", \"num_digits\",\n",
    "    \"num_params\", \"num_slashes\", \"num_question\", \"num_percent\", \"num_special\",\n",
    "    \"domain_length\", \"path_length\", \"has_http\",\n",
    "    \"has_suspicious_kw\"\n",
    "]\n",
    "features_df = pd.DataFrame(features.tolist(), columns=feature_names)\n",
    "\n",
    "# Merge with URL + label\n",
    "final_df = pd.concat([sample_df[\"url\"].reset_index(drop=True),\n",
    "                      features_df,\n",
    "                      sample_df[\"label\"].reset_index(drop=True)], axis=1)\n",
    "print(f\"âœ… Features extracted: {len(feature_names)} features (dropped has_https)\")\n",
    "\n",
    "# ========== Step 3: Prepare Data ==========\n",
    "print(\"\\nðŸ“Š Preparing training data...\")\n",
    "X = final_df.drop(columns=[\"url\", \"label\"])\n",
    "y = final_df[\"label\"].map({\"benign\": 0, \"malicious\": 1})\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(f\"âœ… Data prepared: {len(X_train)} training, {len(X_test)} test samples\")\n",
    "\n",
    "# ========== Step 4: Train Logistic Regression ==========\n",
    "print(\"\\nðŸ¤– Training Logistic Regression model...\")\n",
    "model_v2 = LogisticRegression(max_iter=1000)\n",
    "model_v2.fit(X_train_scaled, y_train)\n",
    "print(\"âœ… Model training completed!\")\n",
    "\n",
    "# ========== Step 5: Evaluate ==========\n",
    "print(\"\\nðŸ“ˆ Evaluating model performance...\")\n",
    "y_pred = model_v2.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… Logistic Regression Accuracy (No HTTPS): {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"coefficient\": model_v2.coef_[0]\n",
    "}).sort_values(by=\"coefficient\", ascending=False)\n",
    "\n",
    "print(\"\\nðŸ”Ž Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# ========== Step 6: Save Models ==========\n",
    "print(\"\\nðŸ’¾ Saving trained models...\")\n",
    "\n",
    "# Save the trained logistic regression model (v2)\n",
    "with open('logistic_model_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(model_v2, f)\n",
    "\n",
    "# Save the scaler (v2)\n",
    "with open('scaler_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save the label encoder mapping (for reference)\n",
    "label_mapping = {\"benign\": 0, \"malicious\": 1}\n",
    "with open('label_encoder_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(label_mapping, f)\n",
    "\n",
    "print(\"âœ… Models saved successfully!\")\n",
    "print(\"- logistic_model_v2.pkl: Trained logistic regression model (no HTTPS)\")\n",
    "print(\"- scaler_v2.pkl: Feature scaler\")\n",
    "print(\"- label_encoder_v2.pkl: Label mapping\")\n",
    "\n",
    "# Display model summary\n",
    "print(f\"\\nðŸ“Š Model 2 Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Model Type: Logistic Regression (No HTTPS)\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features: {len(feature_names)} (dropped has_https)\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"=\" * 40)\n",
    "print(\"ðŸŽ‰ Model 2 training pipeline completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95a2b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Model 3: Improved Logistic Regression Training Pipeline\n",
      "======================================================================\n",
      "ðŸ“‚ Loading dataset...\n",
      "âœ… Dataset loaded: 4000 samples (2500 benign, 1500 malicious)\n",
      "\n",
      "ðŸ”§ Extracting enhanced features...\n",
      "âœ… Features extracted: 21 enhanced features\n",
      "\n",
      "ðŸ“Š Preparing training data with enhanced balancing...\n",
      "âœ… Data prepared: 3200 training, 800 test samples\n",
      "\n",
      "ðŸ¤– Training improved Logistic Regression model...\n",
      "âœ… Model training completed!\n",
      "\n",
      "ðŸ“ˆ Evaluating model performance...\n",
      "âœ… Improved Logistic Regression Accuracy: 0.9463\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96       500\n",
      "           1       0.95      0.90      0.93       300\n",
      "\n",
      "    accuracy                           0.95       800\n",
      "   macro avg       0.95      0.94      0.94       800\n",
      "weighted avg       0.95      0.95      0.95       800\n",
      "\n",
      "\n",
      "ðŸ”Ž Feature Importance (Top 10):\n",
      "                     feature  coefficient  abs_coefficient\n",
      "12                  has_http     2.841601         2.841601\n",
      "10             domain_length     2.194940         2.194940\n",
      "6                num_slashes    -1.611573         1.611573\n",
      "19            num_subdomains     1.187762         1.187762\n",
      "11               path_length    -0.667739         0.667739\n",
      "9                num_special    -0.596543         0.596543\n",
      "2                num_hyphens    -0.587082         0.587082\n",
      "7               num_question    -0.452023         0.452023\n",
      "17  domain_looks_established    -0.341154         0.341154\n",
      "5                 num_params     0.325249         0.325249\n",
      "\n",
      "ðŸ’¾ Saving enhanced models...\n",
      "âœ… Enhanced models saved successfully!\n",
      "- logistic_model_v3.pkl: Enhanced logistic regression model\n",
      "- scaler_v3.pkl: Feature scaler\n",
      "- feature_names_v3.pkl: Feature names for reference\n",
      "- label_encoder_v3.pkl: Label mapping\n",
      "\n",
      "ðŸ“Š Model 3 Summary:\n",
      "==================================================\n",
      "Model Type: Enhanced Logistic Regression\n",
      "Training samples: 3200\n",
      "Test samples: 800\n",
      "Features: 21 (enhanced feature set)\n",
      "Test Accuracy: 0.9463\n",
      "Class weighting: Balanced\n",
      "Regularization: C=0.1 (stronger regularization)\n",
      "==================================================\n",
      "ðŸŽ‰ Model 3 training pipeline completed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL 3: IMPROVED LOGISTIC REGRESSION WITH BALANCED TRAINING DATA\n",
    "# ============================================================================\n",
    "# This cell addresses the fundamental issue: the model is biased against legitimate domains\n",
    "# Improvements:\n",
    "# - Better feature engineering with domain reputation features\n",
    "# - More balanced approach to suspicious keyword detection\n",
    "# - Enhanced feature selection to reduce false positives\n",
    "# - Training data augmentation techniques\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "\n",
    "print(\"ðŸš€ Starting Model 3: Improved Logistic Regression Training Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========== Step 1: Load dataset ==========\n",
    "print(\"ðŸ“‚ Loading dataset...\")\n",
    "file_path = \"features_10000.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "df = df.rename(columns={\"type\": \"label\"})\n",
    "\n",
    "# Balance dataset: 2500 benign, 1500 malicious\n",
    "benign_df = df[df[\"label\"].str.lower() == \"benign\"].head(2500)\n",
    "malicious_df = df[df[\"label\"].str.lower() == \"malicious\"].head(1500)\n",
    "sample_df = pd.concat([benign_df, malicious_df]).reset_index(drop=True)\n",
    "print(f\"âœ… Dataset loaded: {len(sample_df)} samples ({len(benign_df)} benign, {len(malicious_df)} malicious)\")\n",
    "\n",
    "# ========== Step 2: Enhanced Feature Engineering ==========\n",
    "print(\"\\nðŸ”§ Extracting enhanced features...\")\n",
    "def extract_enhanced_features(url):\n",
    "    try:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "    except:\n",
    "        parsed = None\n",
    "\n",
    "    # Basic URL features\n",
    "    url_length = len(url)\n",
    "    num_dots = url.count(\".\")\n",
    "    num_hyphens = url.count(\"-\")\n",
    "    num_at = url.count(\"@\")\n",
    "    num_digits = sum(c.isdigit() for c in url)\n",
    "    num_params = url.count(\"=\")\n",
    "    num_slashes = url.count(\"/\")\n",
    "    num_question = url.count(\"?\")\n",
    "    num_percent = url.count(\"%\")\n",
    "    num_special = sum(c in [';', '_', '?', '=', '&'] for c in url)\n",
    "\n",
    "    hostname = parsed.hostname if parsed and parsed.hostname else \"\"\n",
    "    domain_length = len(hostname)\n",
    "    path_length = len(parsed.path) if parsed and parsed.path else 0\n",
    "    has_http = 1 if parsed and parsed.scheme == \"http\" else 0\n",
    "\n",
    "    # IMPROVED: More nuanced suspicious keyword detection\n",
    "    # Split into different categories with different weights\n",
    "    phishing_keywords = [\"login\", \"secure\", \"verify\", \"account\", \"bank\", \"paypal\", \"update\"]\n",
    "    marketing_keywords = [\"free\", \"win\", \"prize\", \"offer\", \"deal\"]\n",
    "    \n",
    "    has_phishing_kw = 1 if any(kw in url.lower() for kw in phishing_keywords) else 0\n",
    "    has_marketing_kw = 1 if any(kw in url.lower() for kw in marketing_keywords) else 0\n",
    "\n",
    "    # NEW: Domain reputation features\n",
    "    domain_parts = hostname.split('.')\n",
    "    main_domain = domain_parts[0] if domain_parts else \"\"\n",
    "    \n",
    "    # Check for suspicious TLDs\n",
    "    suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.pw']\n",
    "    has_suspicious_tld = 1 if any(hostname.endswith(tld) for tld in suspicious_tlds) else 0\n",
    "    \n",
    "    # Check for legitimate TLDs\n",
    "    legitimate_tlds = ['.com', '.org', '.net', '.edu', '.gov', '.mil']\n",
    "    has_legitimate_tld = 1 if any(hostname.endswith(tld) for tld in legitimate_tlds) else 0\n",
    "    \n",
    "    # Domain age indicator (simple heuristic based on length and structure)\n",
    "    domain_looks_established = 1 if (len(main_domain) >= 4 and \n",
    "                                   not re.match(r'^[0-9]+$', main_domain) and\n",
    "                                   len(domain_parts) >= 2) else 0\n",
    "    \n",
    "    # URL shortener detection\n",
    "    url_shorteners = ['bit.ly', 'tinyurl.com', 'ow.ly', 't.co', 'goo.gl']\n",
    "    is_url_shortener = 1 if any(shortener in hostname for shortener in url_shorteners) else 0\n",
    "    \n",
    "    # Subdomain analysis\n",
    "    num_subdomains = len(domain_parts) - 2 if len(domain_parts) > 2 else 0\n",
    "    \n",
    "    # IP address detection\n",
    "    is_ip_address = 1 if re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+', hostname) else 0\n",
    "    \n",
    "    return [\n",
    "        url_length, num_dots, num_hyphens, num_at, num_digits,\n",
    "        num_params, num_slashes, num_question, num_percent, num_special,\n",
    "        domain_length, path_length, has_http,\n",
    "        has_phishing_kw, has_marketing_kw,  # Split suspicious keywords\n",
    "        has_suspicious_tld, has_legitimate_tld, domain_looks_established,\n",
    "        is_url_shortener, num_subdomains, is_ip_address\n",
    "    ]\n",
    "\n",
    "# Apply enhanced feature extraction\n",
    "features = sample_df[\"url\"].apply(extract_enhanced_features)\n",
    "feature_names = [\n",
    "    \"url_length\", \"num_dots\", \"num_hyphens\", \"num_at\", \"num_digits\",\n",
    "    \"num_params\", \"num_slashes\", \"num_question\", \"num_percent\", \"num_special\",\n",
    "    \"domain_length\", \"path_length\", \"has_http\",\n",
    "    \"has_phishing_kw\", \"has_marketing_kw\",\n",
    "    \"has_suspicious_tld\", \"has_legitimate_tld\", \"domain_looks_established\",\n",
    "    \"is_url_shortener\", \"num_subdomains\", \"is_ip_address\"\n",
    "]\n",
    "features_df = pd.DataFrame(features.tolist(), columns=feature_names)\n",
    "\n",
    "# Merge with URL + label\n",
    "final_df = pd.concat([sample_df[\"url\"].reset_index(drop=True),\n",
    "                      features_df,\n",
    "                      sample_df[\"label\"].reset_index(drop=True)], axis=1)\n",
    "print(f\"âœ… Features extracted: {len(feature_names)} enhanced features\")\n",
    "\n",
    "# ========== Step 3: Data Preparation with Class Balancing ==========\n",
    "print(\"\\nðŸ“Š Preparing training data with enhanced balancing...\")\n",
    "X = final_df.drop(columns=[\"url\", \"label\"])\n",
    "y = final_df[\"label\"].map({\"benign\": 0, \"malicious\": 1})\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(f\"âœ… Data prepared: {len(X_train)} training, {len(X_test)} test samples\")\n",
    "\n",
    "# ========== Step 4: Train Improved Logistic Regression ==========\n",
    "print(\"\\nðŸ¤– Training improved Logistic Regression model...\")\n",
    "# Use class_weight='balanced' to handle class imbalance better\n",
    "model_v3 = LogisticRegression(max_iter=1000, class_weight='balanced', C=0.1)\n",
    "model_v3.fit(X_train_scaled, y_train)\n",
    "print(\"âœ… Model training completed!\")\n",
    "\n",
    "# ========== Step 5: Comprehensive Evaluation ==========\n",
    "print(\"\\nðŸ“ˆ Evaluating model performance...\")\n",
    "y_pred = model_v3.predict(X_test_scaled)\n",
    "y_pred_proba = model_v3.predict_proba(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… Improved Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"coefficient\": model_v3.coef_[0],\n",
    "    \"abs_coefficient\": np.abs(model_v3.coef_[0])\n",
    "}).sort_values(by=\"abs_coefficient\", ascending=False)\n",
    "\n",
    "print(\"\\nðŸ”Ž Feature Importance (Top 10):\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# ========== Step 6: Save Enhanced Models ==========\n",
    "print(\"\\nðŸ’¾ Saving enhanced models...\")\n",
    "\n",
    "# Save the trained logistic regression model (v3)\n",
    "with open('logistic_model_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(model_v3, f)\n",
    "\n",
    "# Save the scaler (v3)\n",
    "with open('scaler_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save feature names for reference\n",
    "with open('feature_names_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_names, f)\n",
    "\n",
    "# Save the label encoder mapping (for reference)\n",
    "label_mapping = {\"benign\": 0, \"malicious\": 1}\n",
    "with open('label_encoder_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(label_mapping, f)\n",
    "\n",
    "print(\"âœ… Enhanced models saved successfully!\")\n",
    "print(\"- logistic_model_v3.pkl: Enhanced logistic regression model\")\n",
    "print(\"- scaler_v3.pkl: Feature scaler\")\n",
    "print(\"- feature_names_v3.pkl: Feature names for reference\")\n",
    "print(\"- label_encoder_v3.pkl: Label mapping\")\n",
    "\n",
    "# Display model summary\n",
    "print(f\"\\nðŸ“Š Model 3 Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model Type: Enhanced Logistic Regression\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features: {len(feature_names)} (enhanced feature set)\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Class weighting: Balanced\")\n",
    "print(f\"Regularization: C=0.1 (stronger regularization)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ðŸŽ‰ Model 3 training pipeline completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
